{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Action Prediction using Deep Multi-Scale Video Prediction Beyond Mean Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is based on predicting future action frames trained on dataset of Human Actions like slide, bend, walk, run, skip etc. The prediction is based on learning to predict future images from a video sequence, where this video sequence is converted into a images and then these images are used for training the Adversarial Network. Convolutional networks have short-range dependencies and thus using an Adversarial network helps in training the network using Multi-scale model. The network makes a series of prediction starting from lower resolution and uses a prediction of s1 to make a prediction for s2 for the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions: \n",
    "1. bce_loss: Calculates the sum of binary cross-entropy losses between predictions and ground truths\n",
    "2. lp_loss: Calculates the sum of lp losses between the predicted and ground truth frames.\n",
    "3. gdl_loss: Calculates the sum of GDL losses between the predicted and ground truth frames. \n",
    "4. adv_loss: Calculates the sum of BCE losses between the predicted classifications and true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert_video_to_jpg.py\n",
    "\n",
    "This python file is responsible for generating images for a mp4 video file. The below example shows to generate images from mulitple videos contained in a directory. All the images of each video are stored in a seperate directory created inside that folder containing video files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading video file: daria_skip.avi\n",
      "creating dir: data/Human_Actions/skip\\daria_skip\n",
      "Reading video file: denis_skip.avi\n",
      "creating dir: data/Human_Actions/skip\\denis_skip\n",
      "Reading video file: eli_skip.avi\n",
      "creating dir: data/Human_Actions/skip\\eli_skip\n",
      "Reading video file: ido_skip.avi\n",
      "creating dir: data/Human_Actions/skip\\ido_skip\n"
     ]
    }
   ],
   "source": [
    "%run convert_video_to_jpg.py --v=data/Human_Actions/skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate_clips.py :\n",
    "\n",
    "Takes input a directory of training images generated from the video clips. The file takes a batch of 5 images from a folder (4 as input images, 1 to be predicted) as  and saves the image as npz (compressed form). These clips are generated for both Training data set and Testing data set. We can specify the directory for which we want to generate clips, where we want to save these clips and the number of clips to be generated. Generally for a good training, we require 100,000 clips for a data set containing 200,000+ images.\n",
    "\n",
    "Below is an example for running the process_data.py to generate 100 clips from the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gettrainframes: data/Human_Actions/Train/\n",
      "call process_training_data with num_clips\n",
      "Directory chosen for generating clips:  ['data/Human_Actions/Train\\\\daria_bend_1']\n",
      "Image selected for compression:  data/Human_Actions/Train\\daria_bend_1\\daria_bend_frame21.jpg\n",
      "Image selected for compression:  data/Human_Actions/Train\\daria_bend_1\\daria_bend_frame22.jpg\n",
      "Image selected for compression:  data/Human_Actions/Train\\daria_bend_1\\daria_bend_frame23.jpg\n",
      "Image selected for compression:  data/Human_Actions/Train\\daria_bend_1\\daria_bend_frame24.jpg\n",
      "Image selected for compression:  data/Human_Actions/Train\\daria_bend_1\\daria_bend_frame25.jpg\n"
     ]
    }
   ],
   "source": [
    "%run generate_clips.py --t=data/Human_Actions/Train/ --c=data/Human_Actions/Clips/ --n=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While generating clips in large amount, the directory structures and files names can be commented from utils.py - get_full_clips() method. The number of recursions or the number of images to be stored in one compressed format can be changed to a bigger number, but due to memory constraints we prefer to train with compressing 4 + 1 images into one clip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main_prediction.py\n",
    "\n",
    "This is the main python file to run the network in training mode or testing mode. In order to test the network, we can run main_prediction.py with test_only mode as shown below. We have specified the test directory for which we want to test predictions. The test_dir must have at least one sub-folder which contains atleast 6-10 images. The --recursions parameter specifies the number of future predictions to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating discriminator...\n",
      "Initiating generator...\n",
      "GENERATOR: batch_size_train:  Tensor(\"generator/data/strided_slice:0\", shape=(), dtype=int32)\n",
      "GENERATOR: batch_size_test:  Tensor(\"generator/data/strided_slice_1:0\", shape=(), dtype=int32)\n",
      "Initiating other variables...\n",
      "Directory chosen for generating clips:  ['data/Human_Actions/NewTest_3\\\\idio_skip_1'\n",
      " 'data/Human_Actions/NewTest_3\\\\person16_running_1'\n",
      " 'data/Human_Actions/NewTest_3\\\\idio_skip_1'\n",
      " 'data/Human_Actions/NewTest_3\\\\idio_skip_1'\n",
      " 'data/Human_Actions/NewTest_3\\\\idio_skip_1'\n",
      " 'data/Human_Actions/NewTest_3\\\\person18_handclapping_1'\n",
      " 'data/Human_Actions/NewTest_3\\\\person16_running_1'\n",
      " 'data/Human_Actions/NewTest_3\\\\person18_handclapping_1']\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame14.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame15.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame16.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame17.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame18.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person16_running_1\\person16_running_d4_uncomp_frame21.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person16_running_1\\person16_running_d4_uncomp_frame22.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person16_running_1\\person16_running_d4_uncomp_frame23.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person16_running_1\\person16_running_d4_uncomp_frame24.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person16_running_1\\person16_running_d4_uncomp_frame25.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame15.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame16.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame17.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame18.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame19.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame13.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame14.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame15.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame16.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame17.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame14.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame15.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame16.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame17.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\idio_skip_1\\ido_skip_frame18.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person18_handclapping_1\\person18_handclapping_d1_uncomp_frame15.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person18_handclapping_1\\person18_handclapping_d1_uncomp_frame16.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person18_handclapping_1\\person18_handclapping_d1_uncomp_frame17.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person18_handclapping_1\\person18_handclapping_d1_uncomp_frame18.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person18_handclapping_1\\person18_handclapping_d1_uncomp_frame19.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person16_running_1\\person16_running_d4_uncomp_frame21.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person16_running_1\\person16_running_d4_uncomp_frame22.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person16_running_1\\person16_running_d4_uncomp_frame23.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person16_running_1\\person16_running_d4_uncomp_frame24.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person16_running_1\\person16_running_d4_uncomp_frame25.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person18_handclapping_1\\person18_handclapping_d1_uncomp_frame12.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person18_handclapping_1\\person18_handclapping_d1_uncomp_frame13.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person18_handclapping_1\\person18_handclapping_d1_uncomp_frame14.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person18_handclapping_1\\person18_handclapping_d1_uncomp_frame15.jpg\n",
      "Image selected for compression:  data/Human_Actions/NewTest_3\\person18_handclapping_1\\person18_handclapping_d1_uncomp_frame16.jpg\n",
      "------------------------------\n",
      "Testing with GlobalStep  0\n",
      "Testing:\n",
      "Recursion  0\n",
      "PSNR Error     :  9.81192\n",
      "Sharpdiff Error:  11.7034\n",
      "------------------------------\n",
      "data/Human_Actions/Save/Images/Default/Tests/NewTest_3/Step_0\n"
     ]
    }
   ],
   "source": [
    "%run main_prediction.py --test_dir=data/Human_Actions/NewTest_3 --recursions=1 --test_only\n",
    "\n",
    "# the c.GIF_SRC_FOLDER contains the latest GIF created by running the above test mode.\n",
    "import constants as c\n",
    "print(c.GIF_SRC_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the below script to fetch the last generated GIFs from testing the above directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GIF:       data/Human_Actions/Save/Images/Default/Tests/NewTest_3/Step_0/0/originalInput_GIF.gif\n",
      "Ground Truth GIF:   data/Human_Actions/Save/Images/Default/Tests/NewTest_3/Step_0/0/ogt_GIF.gif\n",
      "Generated GIF:      data/Human_Actions/Save/Images/Default/Tests/NewTest_3/Step_0/0/ogen_GIF.gif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import constants as c\n",
    "\n",
    "original_filename = ''\n",
    "ogt_filename = ''\n",
    "ogen_filename = ''\n",
    "        \n",
    "count = 0\n",
    "\n",
    "for root, dirs, filenames in os.walk(c.GIF_SRC_FOLDER):\n",
    "    #print('root: ', root)\n",
    "    #print('dirs: ', dirs)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        fullFileName = os.path.join(root,filename)\n",
    "        if \"original\" in filename:\n",
    "            count += 1\n",
    "            original_filename = fullFileName\n",
    "            #print('original gif: ', fullFileName)\n",
    "        elif \"ogt\" in filename:\n",
    "            ogt_filename = fullFileName\n",
    "            count += 1\n",
    "            #print('ogt gif: ', fullFileName)\n",
    "        elif \"ogen\" in filename:\n",
    "            ogen_filename = fullFileName\n",
    "            count += 1\n",
    "            #print('ogen gif: ', fullFileName)\n",
    "            \n",
    "    if not dirs:\n",
    "        if count == 3:\n",
    "            original_filename = original_filename.replace('\\\\','/')\n",
    "            ogt_filename = ogt_filename.replace('\\\\','/')\n",
    "            ogen_filename = ogen_filename.replace('\\\\','/')\n",
    "            print('Original GIF:      ', original_filename)\n",
    "            print('Ground Truth GIF:  ', ogt_filename)\n",
    "            print('Generated GIF:     ', ogen_filename)\n",
    "            \n",
    "            break\n",
    "        #image = load_image(original_filename)\n",
    "        #plot_image(image)\n",
    "        \n",
    "        #image = load_image(ogt_filename)\n",
    "        #plot_image(image)\n",
    "        \n",
    "        #image = load_image(ogen_filename)\n",
    "        #plot_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Original GIF, Ground Truth GIF, Generated GIF are the generated output files from the above test scripts. Running the below script, we can visually see all the images - Orginal Frames, Ground Truth and Generated -  the ground truth and generated GIFs have the original frame added in their frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Originial:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br><img id='myOrImg' src='data/Human_Actions/Save/Images/Default/Tests/NewTest_3/Step_0/0/originalInput_GIF.gif'/><td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Ground Truth:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><img id='myGtImg' src='data/Human_Actions/Save/Images/Default/Tests/NewTest_3/Step_0/0/ogt_GIF.gif' /><td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Generated:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><img id='myGenImg' src='data/Human_Actions/Save/Images/Default/Tests/NewTest_3/Step_0/0/ogen_GIF.gif' /></td></tr></table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "HTML(\"<div id='textid'>\" + original_filename)\n",
    "HTML(\"<table><tr><td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Originial:</B>\"+\n",
    "     \"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>\"+\n",
    "     \"<img id='myOrImg' src='\"+original_filename +\"'/>\"\n",
    "     \"<td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Ground Truth:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>\"\n",
    "     +\"<img id='myGtImg' src='\"+ogt_filename+\"' />\"\n",
    "     +\"<td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Generated:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>\"\n",
    "     +\"<img id='myGenImg' src='\"+ogen_filename+\"' />\"\n",
    "     +\"</td></tr></table></div>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Originial:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br><img id='myOrImg' src='data/Human_Actions/NewTest_2/Step_0/0/originalInput_GIF.gif' /><td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Ground Truth:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><img id='myGtImg' src='data/Human_Actions/NewTest_2/Step_0/0/ogt_GIF.gif' /><td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Generated:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br><img id='myGenImg' src='data/Human_Actions/NewTest_2/Step_0/0/ogen_GIF.gif' /></td></tr></table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"<div id='textid'>\" + original_filename)\n",
    "HTML(\"<table><tr><td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Originial:</B>\"+\n",
    "     \"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>\"+\n",
    "     \"<img id='myOrImg' src='data/Human_Actions/NewTest_2/Step_0/0/originalInput_GIF.gif' />\"\n",
    "     \"<td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Ground Truth:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>\"\n",
    "     +\"<img id='myGtImg' src='data/Human_Actions/NewTest_2/Step_0/0/ogt_GIF.gif' />\"\n",
    "     +\"<td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Generated:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>\"\n",
    "     +\"<img id='myGenImg' src='data/Human_Actions/NewTest_2/Step_0/0/ogen_GIF.gif' />\"\n",
    "     +\"</td></tr></table></div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"<div id='textid'>\" + original_filename)\n",
    "HTML(\"<table><tr>\"\n",
    "     +\"<td>Input_0<img src='data/Human_Actions/NewTest_2/Step_0/1/input_0.png' /></td>\"\n",
    "     +\"<td>Input_1<img src='data/Human_Actions/NewTest_2/Step_0/1/input_1.png' /></td>\"\n",
    "     +\"<td>Input_2<img src='data/Human_Actions/NewTest_2/Step_0/1/input_2.png' /></td>\"\n",
    "     +\"<td>Input_3<img src='data/Human_Actions/NewTest_2/Step_0/1/input_3.png' /></td></tr>\"\n",
    "     +\"<tr><td>Ground Truth\"\n",
    "     +\"<img id='myGtImg' src='data/Human_Actions/NewTest_2/Step_0/1/gt_0.png' /></td></tr>\"\n",
    "     +\"<tr><td>Generated\"\n",
    "     +\"<img id='myGenImg' src='data/Human_Actions/NewTest_2/Step_0/1/gen_0.png' /></td></tr>\"\n",
    "     +\"</table></div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"<div id='textid'>\" + original_filename)\n",
    "HTML(\"<table><tr>\"\n",
    "     +\"<td>Input_0<img src='data/Human_Actions/NewTest_2/Step_0_45/2/input_0.png' /></td>\"\n",
    "     +\"<td>Input_1<img src='data/Human_Actions/NewTest_2/Step_0_45/2/input_1.png' /></td>\"\n",
    "     +\"<td>Input_2<img src='data/Human_Actions/NewTest_2/Step_0_45/2/input_2.png' /></td>\"\n",
    "     +\"<td>Input_3<img src='data/Human_Actions/NewTest_2/Step_0_45/2/input_3.png' /></td></tr>\"\n",
    "     +\"<tr>\"\n",
    "     +\"<td>Ground Truth_0<img src='data/Human_Actions/NewTest_2/Step_0_45/2/gt_0.png' /></td>\"\n",
    "     +\"<td>Ground Truth_1<img src='data/Human_Actions/NewTest_2/Step_0_45/2/gt_1.png' /></td>\"\n",
    "     +\"<td>Ground Truth_2<img src='data/Human_Actions/NewTest_2/Step_0_45/2/gt_2.png' /></td></tr>\"\n",
    "     +\"<tr>\"\n",
    "     +\"<td>Generated_0<img src='data/Human_Actions/NewTest_2/Step_0_45/2/gen_0.png' /></td>\"\n",
    "     +\"<td>Generated_1<img src='data/Human_Actions/NewTest_2/Step_0_45/2/gen_1.png' /></td>\"\n",
    "     +\"<td>Generated_2<img src='data/Human_Actions/NewTest_2/Step_0_45/2/gen_2.png' /></td></tr>\"\n",
    "     +\"</table></div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"<div id='textid'>\" + original_filename)\n",
    "HTML(\"<table><tr>\"\n",
    "     +\"<td>Input_0<img src='data/Human_Actions/recursions_4/1/input_0.png' /></td>\"\n",
    "     +\"<td>Input_1<img src='data/Human_Actions/recursions_4/1/input_1.png' /></td>\"\n",
    "     +\"<td>Input_2<img src='data/Human_Actions/recursions_4/1/input_2.png' /></td>\"\n",
    "     +\"<td>Input_3<img src='data/Human_Actions/recursions_4/1/input_3.png' /></td></tr>\"\n",
    "     +\"<tr>\"\n",
    "     +\"<td>Ground Truth_0<img src='data/Human_Actions/recursions_4/1/gt_0.png' /></td>\"\n",
    "     +\"<td>Ground Truth_1<img src='data/Human_Actions/recursions_4/1/gt_1.png' /></td>\"\n",
    "     +\"<td>Ground Truth_2<img src='data/Human_Actions/recursions_4/1/gt_2.png' /></td>\"\n",
    "     +\"<td>Ground Truth_3<img src='data/Human_Actions/recursions_4/1/gt_3.png' /></td></tr>\"\n",
    "     +\"<tr>\"\n",
    "     +\"<td>Generated_0<img src='data/Human_Actions/recursions_4/1/gen_0.png' /></td>\"\n",
    "     +\"<td>Generated_1<img src='data/Human_Actions/recursions_4/1/gen_1.png' /></td>\"\n",
    "     +\"<td>Generated_2<img src='data/Human_Actions/recursions_4/1/gen_2.png' /></td>\"\n",
    "     +\"<td>Generated_3<img src='data/Human_Actions/recursions_4/1/gen_3.png' /></td></tr>\"\n",
    "     +\"</table></div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow3]",
   "language": "python",
   "name": "conda-env-tensorflow3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
