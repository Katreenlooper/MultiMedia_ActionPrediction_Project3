{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Action Prediction using Deep Multi-Scale Video Prediction Beyond Mean Square Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is based on predicting future action frames trained on dataset of Human Actions like slide, bend, walk, run, skip etc. The prediction is based on learning to predict future images from a video sequence, where this video sequence is converted into a images and then these images are used for training the Adversarial Network. Convolutional networks have short-range dependencies and thus using an Adversarial network helps in training the network using Multi-scale model. The network makes a series of prediction starting from lower resolution and uses a prediction of s1 to make a prediction for s2 for the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions: \n",
    "1. bce_loss: Calculates the sum of binary cross-entropy losses between predictions and ground truths\n",
    "2. lp_loss: Calculates the sum of lp losses between the predicted and ground truth frames.\n",
    "3. gdl_loss: Calculates the sum of GDL losses between the predicted and ground truth frames. \n",
    "4. adv_loss: Calculates the sum of BCE losses between the predicted classifications and true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert_video_to_jpg.py\n",
    "\n",
    "This python file is responsible for generating images for a mp4 video file. The below example shows to generate images from mulitple videos contained in a directory. All the images of each video are stored in a seperate directory created inside that folder containing video files. If you have cloned the repository, you can find the folders: skip and walk which contains multiple videos. In order to generate their images, below command can be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run convert_video_to_jpg.py --v=data/Human_Actions/skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate_clips.py :\n",
    "\n",
    "Takes input a directory of training images generated from the video clips. The file takes a batch of 5 images from a folder (4 as input images, 1 to be predicted) as  and saves the image as npz (compressed form). These clips are generated for both Training data set and Testing data set. We can specify the directory for which we want to generate clips, where we want to save these clips and the number of clips to be generated. Generally for a good training, we require 100,000 clips for a data set containing 200,000+ images.\n",
    "\n",
    "Below is an example for running the generate_clips.py to generate 100 clips from the training data set. The github repository has some images in data/Human_Actions/Train and data/Human_Actions/Test, below command can be executed to test how the clips are generated from randomly selected images within the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run generate_clips.py --t=data/Human_Actions/Train/ --c=data/Human_Actions/Clips/ --n=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While generating clips in large amount, the directory structures and files names can be commented from utils.py - get_full_clips() method. The number of recursions or the number of images to be stored in one compressed format can be changed to a bigger number, but due to memory constraints we prefer to train with compressing 4 + 1 images into one clip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main_prediction.py\n",
    "\n",
    "This is the main python file to run the network in training mode or testing mode. In order to test the network, we can run main_prediction.py with test_only mode as shown below. We have specified the test directory for which we want to test predictions. The test_dir must have at least one sub-folder which contains atleast 6-10 images. The --recursions parameter specifies the number of future predictions to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to train the network - one can specify the Training folder.\n",
    "# %run main_prediction.py --test_dir=data/Human_Actions/Train\n",
    "\n",
    "%run main_prediction.py --test_dir=data/Human_Actions/NewTest_3 --recursions=1 --test_only\n",
    "\n",
    "# the c.GIF_SRC_FOLDER contains the latest GIF created by running the above test mode.\n",
    "import constants as c\n",
    "print(c.GIF_SRC_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to test the network with some custom created test folder containing multiple subfolder and containing some images, we can select the folder from this UI and running the below cells, we can generate the output predictions. Make sure that the test folder for which you want to perform testing is inside the Code folder only and nowhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "input_form = \"\"\"\n",
    "\n",
    "<div style=\"background-color:gainsboro; border:solid black; width:800px; padding:20px;\">\n",
    "<br>\n",
    "<input type=\"file\" id=\"file\" onchange=\"getfolder(event)\" webkitdirectory mozdirectory msdirectory odirectory directory multiple />\n",
    "<br>\n",
    "Set Recursions:\n",
    "<select name = \"Recursions\" id=\"Recursions\" value = \"1\" style=\"width: 50px;height:25px\">\n",
    "    <option value = \"1\">1</option>\n",
    "    <option value = \"2\">2</option>\n",
    "    <option value = \"3\">3</option>\n",
    "    <option value = \"4\">4</option>\n",
    "\n",
    "</select>\n",
    "<br>\n",
    "<button onclick=\"getValues()\">Set Parameters</button><br> <br>\n",
    "\n",
    "<span id=\"output\"></span>\n",
    "\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "javascript = \"\"\"\n",
    "<script type=\"text/Javascript\">\n",
    "\n",
    "    \n",
    "    \n",
    "    function getfolder(e) {\n",
    "    var files = e.target.files;\n",
    "    var path = files[0].webkitRelativePath;\n",
    "    var Folder = path.split(\"/\");\n",
    "    var filename = 'file';\n",
    "         \n",
    "    var kernel = IPython.notebook.kernel;\n",
    "    var command = filename +\" = '\" + Folder[0] + \"'\";\n",
    "    kernel.execute(command);\n",
    "    \n",
    "}\n",
    "    function getValues(){\n",
    "    var rec = document.getElementById('Recursions').value;\n",
    "    var recur = 'recur';\n",
    "    var kernel = IPython.notebook.kernel;\n",
    "    var command = recur +\" = '\" + rec + \"'\";\n",
    "    kernel.execute(command);\n",
    "    }\n",
    "   \n",
    "</script>\n",
    "\"\"\"\n",
    "        \n",
    "\n",
    "HTML(input_form + javascript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting the parameters - selecting the test folder and number of recursions, execute the below cell to perform testing. This will run the main_prediction.py in test_mode and generate the next predictions as specified in the Set Parameters for the test directory selected by you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fi= \"--test_dir=\"+file\n",
    "print(fi)\n",
    "re=\"--recursions=\"+recur\n",
    "print(re)\n",
    "%run main_prediction.py $fi $re --test_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the below script to fetch the last generated GIFs from testing the above directory.\n",
    "In order to check the output predictions, execute the below cell and you can see the Original input, the Ground Truth and the Generated Predictions GIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import constants as c\n",
    "\n",
    "original_filename = ''\n",
    "ogt_filename = ''\n",
    "ogen_filename = ''\n",
    "        \n",
    "count = 0\n",
    "\n",
    "for root, dirs, filenames in os.walk(c.GIF_SRC_FOLDER):\n",
    "    #print('root: ', root)\n",
    "    #print('dirs: ', dirs)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        fullFileName = os.path.join(root,filename)\n",
    "        if \"original\" in filename:\n",
    "            count += 1\n",
    "            original_filename = fullFileName\n",
    "            #print('original gif: ', fullFileName)\n",
    "        elif \"ogt\" in filename:\n",
    "            ogt_filename = fullFileName\n",
    "            count += 1\n",
    "            #print('ogt gif: ', fullFileName)\n",
    "        elif \"ogen\" in filename:\n",
    "            ogen_filename = fullFileName\n",
    "            count += 1\n",
    "            #print('ogen gif: ', fullFileName)\n",
    "            \n",
    "    if not dirs:\n",
    "        if count == 3:\n",
    "            original_filename = original_filename.replace('\\\\','/')\n",
    "            ogt_filename = ogt_filename.replace('\\\\','/')\n",
    "            ogen_filename = ogen_filename.replace('\\\\','/')\n",
    "            print('Original GIF:      ', original_filename)\n",
    "            print('Ground Truth GIF:  ', ogt_filename)\n",
    "            print('Generated GIF:     ', ogen_filename)\n",
    "            \n",
    "            break\n",
    "        #image = load_image(original_filename)\n",
    "        #plot_image(image)\n",
    "        \n",
    "        #image = load_image(ogt_filename)\n",
    "        #plot_image(image)\n",
    "        \n",
    "        #image = load_image(ogen_filename)\n",
    "        #plot_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the Input, Ground Truth and Generated GIFs\n",
    "The Original GIF, Ground Truth GIF, Generated GIF are the generated output files from the above test scripts. Running the below script, we can visually see all the images - Orginal Frames, Ground Truth and Generated -  the ground truth and generated GIFs have the original frame added in their frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>Input_0<img src='data/Human_Actions/recursions_4/1/input_0.png' /></td><td>Input_1<img src='data/Human_Actions/recursions_4/1/input_1.png' /></td><td>Input_2<img src='data/Human_Actions/recursions_4/1/input_2.png' /></td><td>Input_3<img src='data/Human_Actions/recursions_4/1/input_3.png' /></td></tr><tr><td>Ground Truth_0<img src='data/Human_Actions/recursions_4/1/gt_0.png' /></td><td>Ground Truth_1<img src='data/Human_Actions/recursions_4/1/gt_1.png' /></td><td>Ground Truth_2<img src='data/Human_Actions/recursions_4/1/gt_2.png' /></td><td>Ground Truth_3<img src='data/Human_Actions/recursions_4/1/gt_3.png' /></td></tr><tr><td>Generated_0<img src='data/Human_Actions/recursions_4/1/gen_0.png' /></td><td>Generated_1<img src='data/Human_Actions/recursions_4/1/gen_1.png' /></td><td>Generated_2<img src='data/Human_Actions/recursions_4/1/gen_2.png' /></td><td>Generated_3<img src='data/Human_Actions/recursions_4/1/gen_3.png' /></td></tr></table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"<div id='textid'>\" + original_filename)\n",
    "HTML(\"<table><tr>\"\n",
    "     +\"<td>Input_0<img src='data/Human_Actions/recursions_4/1/input_0.png' /></td>\"\n",
    "     +\"<td>Input_1<img src='data/Human_Actions/recursions_4/1/input_1.png' /></td>\"\n",
    "     +\"<td>Input_2<img src='data/Human_Actions/recursions_4/1/input_2.png' /></td>\"\n",
    "     +\"<td>Input_3<img src='data/Human_Actions/recursions_4/1/input_3.png' /></td></tr>\"\n",
    "     +\"<tr>\"\n",
    "     +\"<td>Ground Truth_0<img src='data/Human_Actions/recursions_4/1/gt_0.png' /></td>\"\n",
    "     +\"<td>Ground Truth_1<img src='data/Human_Actions/recursions_4/1/gt_1.png' /></td>\"\n",
    "     +\"<td>Ground Truth_2<img src='data/Human_Actions/recursions_4/1/gt_2.png' /></td>\"\n",
    "     +\"<td>Ground Truth_3<img src='data/Human_Actions/recursions_4/1/gt_3.png' /></td></tr>\"\n",
    "     +\"<tr>\"\n",
    "     +\"<td>Generated_0<img src='data/Human_Actions/recursions_4/1/gen_0.png' /></td>\"\n",
    "     +\"<td>Generated_1<img src='data/Human_Actions/recursions_4/1/gen_1.png' /></td>\"\n",
    "     +\"<td>Generated_2<img src='data/Human_Actions/recursions_4/1/gen_2.png' /></td>\"\n",
    "     +\"<td>Generated_3<img src='data/Human_Actions/recursions_4/1/gen_3.png' /></td></tr>\"\n",
    "     +\"</table></div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "HTML(\"<div id='textid'>\" + original_filename)\n",
    "HTML(\"<table><tr><td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Originial:</B>\"+\n",
    "     \"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>\"+\n",
    "     \"<img id='myOrImg' src='\"+original_filename +\"'/>\"\n",
    "     \"<td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Ground Truth:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>\"\n",
    "     +\"<img id='myGtImg' src='\"+ogt_filename+\"' />\"\n",
    "     +\"<td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Generated:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>\"\n",
    "     +\"<img id='myGenImg' src='\"+ogen_filename+\"' />\"\n",
    "     +\"</td></tr></table></div>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other test results:\n",
    "\n",
    "Other results as produced by our network are plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"<div id='textid'>\" + original_filename)\n",
    "HTML(\"<table><tr><td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Originial:</B>\"+\n",
    "     \"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>\"+\n",
    "     \"<img id='myOrImg' src='data/Human_Actions/NewTest_2/Step_0/0/originalInput_GIF.gif' />\"\n",
    "     \"<td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Ground Truth:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>\"\n",
    "     +\"<img id='myGtImg' src='data/Human_Actions/NewTest_2/Step_0/0/ogt_GIF.gif' />\"\n",
    "     +\"<td>&nbsp;&nbsp;&nbsp;&nbsp;<B>Generated:</B>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>\"\n",
    "     +\"<img id='myGenImg' src='data/Human_Actions/NewTest_2/Step_0/0/ogen_GIF.gif' />\"\n",
    "     +\"</td></tr></table></div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"<div id='textid'>\" + original_filename)\n",
    "HTML(\"<table><tr>\"\n",
    "     +\"<td>Input_0<img src='data/Human_Actions/NewTest_2/Step_0/1/input_0.png' /></td>\"\n",
    "     +\"<td>Input_1<img src='data/Human_Actions/NewTest_2/Step_0/1/input_1.png' /></td>\"\n",
    "     +\"<td>Input_2<img src='data/Human_Actions/NewTest_2/Step_0/1/input_2.png' /></td>\"\n",
    "     +\"<td>Input_3<img src='data/Human_Actions/NewTest_2/Step_0/1/input_3.png' /></td></tr>\"\n",
    "     +\"<tr><td>Ground Truth\"\n",
    "     +\"<img id='myGtImg' src='data/Human_Actions/NewTest_2/Step_0/1/gt_0.png' /></td></tr>\"\n",
    "     +\"<tr><td>Generated\"\n",
    "     +\"<img id='myGenImg' src='data/Human_Actions/NewTest_2/Step_0/1/gen_0.png' /></td></tr>\"\n",
    "     +\"</table></div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HTML(\"<div id='textid'>\" + original_filename)\n",
    "HTML(\"<table><tr>\"\n",
    "     +\"<td>Input_0<img src='data/Human_Actions/NewTest_2/Step_0_45/2/input_0.png' /></td>\"\n",
    "     +\"<td>Input_1<img src='data/Human_Actions/NewTest_2/Step_0_45/2/input_1.png' /></td>\"\n",
    "     +\"<td>Input_2<img src='data/Human_Actions/NewTest_2/Step_0_45/2/input_2.png' /></td>\"\n",
    "     +\"<td>Input_3<img src='data/Human_Actions/NewTest_2/Step_0_45/2/input_3.png' /></td></tr>\"\n",
    "     +\"<tr>\"\n",
    "     +\"<td>Ground Truth_0<img src='data/Human_Actions/NewTest_2/Step_0_45/2/gt_0.png' /></td>\"\n",
    "     +\"<td>Ground Truth_1<img src='data/Human_Actions/NewTest_2/Step_0_45/2/gt_1.png' /></td>\"\n",
    "     +\"<td>Ground Truth_2<img src='data/Human_Actions/NewTest_2/Step_0_45/2/gt_2.png' /></td></tr>\"\n",
    "     +\"<tr>\"\n",
    "     +\"<td>Generated_0<img src='data/Human_Actions/NewTest_2/Step_0_45/2/gen_0.png' /></td>\"\n",
    "     +\"<td>Generated_1<img src='data/Human_Actions/NewTest_2/Step_0_45/2/gen_1.png' /></td>\"\n",
    "     +\"<td>Generated_2<img src='data/Human_Actions/NewTest_2/Step_0_45/2/gen_2.png' /></td></tr>\"\n",
    "     +\"</table></div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"<div id='textid'>\" + original_filename)\n",
    "HTML(\"<table><tr>\"\n",
    "     +\"<td>Input_0<img src='data/Human_Actions/NewTest_2/Step_0_45/4/input_0.png' /></td>\"\n",
    "     +\"<td>Input_1<img src='data/Human_Actions/NewTest_2/Step_0_45/4/input_1.png' /></td>\"\n",
    "     +\"<td>Input_2<img src='data/Human_Actions/NewTest_2/Step_0_45/4/input_2.png' /></td>\"\n",
    "     +\"<td>Input_3<img src='data/Human_Actions/NewTest_2/Step_0_45/4/input_3.png' /></td></tr>\"\n",
    "     +\"<tr>\"\n",
    "     +\"<td>Ground Truth_0<img src='data/Human_Actions/NewTest_2/Step_0_45/4/gt_0.png' /></td>\"\n",
    "     +\"<td>Ground Truth_2<img src='data/Human_Actions/NewTest_2/Step_0_45/4/gt_1.png' /></td></tr>\"\n",
    "     +\"<tr>\"\n",
    "     +\"<td>Generated_0<img src='data/Human_Actions/NewTest_2/Step_0_45/4/gen_0.png' /></td>\"\n",
    "     +\"<td>Generated_2<img src='data/Human_Actions/NewTest_2/Step_0_45/4/gen_1.png' /></td></tr>\"\n",
    "     +\"</table></div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"<div id='textid'>\" + original_filename)\n",
    "HTML(\"<table><tr>\"\n",
    "     +\"<td>Input_0<img src='data/Human_Actions/recursions_4/0/input_0.png' /></td>\"\n",
    "     +\"<td>Input_1<img src='data/Human_Actions/recursions_4/0/input_1.png' /></td>\"\n",
    "     +\"<td>Input_2<img src='data/Human_Actions/recursions_4/0/input_2.png' /></td>\"\n",
    "     +\"<td>Input_3<img src='data/Human_Actions/recursions_4/0/input_3.png' /></td></tr>\"\n",
    "     +\"<tr>\"\n",
    "     +\"<td>Ground Truth_0<img src='data/Human_Actions/recursions_4/0/gt_0.png' /></td>\"\n",
    "     +\"<td>Ground Truth_1<img src='data/Human_Actions/recursions_4/0/gt_1.png' /></td>\"\n",
    "     +\"<td>Ground Truth_2<img src='data/Human_Actions/recursions_4/0/gt_2.png' /></td>\"\n",
    "     +\"<td>Ground Truth_3<img src='data/Human_Actions/recursions_4/0/gt_3.png' /></td></tr>\"\n",
    "     +\"<tr>\"\n",
    "     +\"<td>Generated_0<img src='data/Human_Actions/recursions_4/0/gen_0.png' /></td>\"\n",
    "     +\"<td>Generated_1<img src='data/Human_Actions/recursions_4/0/gen_1.png' /></td>\"\n",
    "     +\"<td>Generated_2<img src='data/Human_Actions/recursions_4/0/gen_2.png' /></td>\"\n",
    "     +\"<td>Generated_3<img src='data/Human_Actions/recursions_4/0/gen_3.png' /></td></tr>\"\n",
    "     +\"</table></div>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow3]",
   "language": "python",
   "name": "conda-env-tensorflow3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
